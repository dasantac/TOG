{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362e4bb8",
   "metadata": {},
   "source": [
    "# Context\n",
    "This notebook drives the training process for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8557528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project's environment variables\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../../project.env\")\n",
    "sys.path.append(os.environ[\"PYTHONPATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8f9643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chosen class grouping: two-classes\n",
      "\n",
      "\n",
      "Directory /Users/diego/Desktop/iteso/TOG/ exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/data exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/src exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/bin exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/media exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/scores exists. Continuing with execution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/miniconda3/envs/TOG_BERT_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /Users/diego/Desktop/iteso/TOG/bin/load/TRAIN/distilbert-base-uncased exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/bin/load/TRAIN/prajjwal1/bert-tiny exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/bin/load/TRAIN/bert-base-uncased exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/bin/load/TRAIN/bert-large-uncased exists. Continuing with execution\n"
     ]
    }
   ],
   "source": [
    "# Import project-wide and PH2 specific variables and functions\n",
    "import superheader as sup\n",
    "import TRAIN.architecture.archeader as arch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ee7ff",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a87327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /Users/diego/Desktop/iteso/TOG/data/PH1/two-classes exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/data/PH2/two-classes exists. Continuing with execution\n",
      "Directory /Users/diego/Desktop/iteso/TOG/data/PH3/two-classes exists. Continuing with execution\n"
     ]
    }
   ],
   "source": [
    "sup.report_dir_if_not_exists(sup.PH1_DATA_ROOT)\n",
    "sup.report_dir_if_not_exists(sup.PH2_DATA_ROOT)\n",
    "sup.report_dir_if_not_exists(sup.PH3_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6506238",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14be9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6657714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cuda\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d01f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "  \"PH2\" : True,\n",
    "  \"PH3\" : False,\n",
    "  \"reducer\" : None,\n",
    "  \"kernel\" : None,\n",
    "  \"n\" : -1,\n",
    "  \"data_unit\" : sup.DATA_S_PV,\n",
    "  \"label_col\" : sup.class_numeric_column,\n",
    "  \"class_list\" : \"two-classes\",\n",
    "  \"test_ratio\" : 0.2\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "  \"device\" : device,\n",
    "  \"input_dim\" : 87,\n",
    "  \"batch_size\" : 213,\n",
    "  \"optimizer\" : optim.AdamW,\n",
    "  \"lr\" : 2e-5,\n",
    "  \"loss_fn\" : nn.CrossEntropyLoss,\n",
    "  \"num_epochs\" : 1000,\n",
    "  \"loadable\" : arch.bert.BERT_TINY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132a3ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/Desktop/iteso/TOG/src/TRAIN/architecture/archeader.py:97: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  spv_df = spf_df.groupby(sup.tag_columns+sup.class_columns).apply(self.__flatten_group).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "model = arch.BERT(data_config=data_config, df=None, train_config=train_config, model_path_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd25e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.2344\n",
      "Epoch 2 Loss: 4.2122\n",
      "Epoch 3 Loss: 4.1784\n",
      "Epoch 4 Loss: 4.1599\n",
      "Epoch 5 Loss: 4.1297\n",
      "Epoch 6 Loss: 4.0857\n",
      "Epoch 7 Loss: 4.0629\n",
      "Epoch 8 Loss: 4.0431\n",
      "Epoch 9 Loss: 4.0088\n",
      "Epoch 10 Loss: 3.9931\n",
      "Epoch 11 Loss: 3.9510\n",
      "Epoch 12 Loss: 3.9261\n",
      "Epoch 13 Loss: 3.8915\n",
      "Epoch 14 Loss: 3.8698\n",
      "Epoch 15 Loss: 3.8491\n",
      "Epoch 16 Loss: 3.8251\n",
      "Epoch 17 Loss: 3.8020\n",
      "Epoch 18 Loss: 3.7900\n",
      "Epoch 19 Loss: 3.7406\n",
      "Epoch 20 Loss: 3.7411\n",
      "Epoch 21 Loss: 3.7139\n",
      "Epoch 22 Loss: 3.6691\n",
      "Epoch 23 Loss: 3.6477\n",
      "Epoch 24 Loss: 3.6209\n",
      "Epoch 25 Loss: 3.5922\n",
      "Epoch 26 Loss: 3.5870\n",
      "Epoch 27 Loss: 3.5470\n",
      "Epoch 28 Loss: 3.5356\n",
      "Epoch 29 Loss: 3.5110\n",
      "Epoch 30 Loss: 3.4928\n",
      "Epoch 31 Loss: 3.4575\n",
      "Epoch 32 Loss: 3.4482\n",
      "Epoch 33 Loss: 3.4125\n",
      "Epoch 34 Loss: 3.3982\n",
      "Epoch 35 Loss: 3.3736\n",
      "Epoch 36 Loss: 3.3575\n",
      "Epoch 37 Loss: 3.3141\n",
      "Epoch 38 Loss: 3.3124\n",
      "Epoch 39 Loss: 3.2778\n",
      "Epoch 40 Loss: 3.2583\n",
      "Epoch 41 Loss: 3.2509\n",
      "Epoch 42 Loss: 3.2256\n",
      "Epoch 43 Loss: 3.2181\n",
      "Epoch 44 Loss: 3.1840\n",
      "Epoch 45 Loss: 3.1666\n",
      "Epoch 46 Loss: 3.1436\n",
      "Epoch 47 Loss: 3.1300\n",
      "Epoch 48 Loss: 3.1115\n",
      "Epoch 49 Loss: 3.0913\n",
      "Epoch 50 Loss: 3.0632\n",
      "Epoch 51 Loss: 3.0709\n",
      "Epoch 52 Loss: 3.0489\n",
      "Epoch 53 Loss: 3.0143\n",
      "Epoch 54 Loss: 2.9984\n",
      "Epoch 55 Loss: 2.9935\n",
      "Epoch 56 Loss: 2.9924\n",
      "Epoch 57 Loss: 2.9532\n",
      "Epoch 58 Loss: 2.9345\n",
      "Epoch 59 Loss: 2.9365\n",
      "Epoch 60 Loss: 2.9164\n",
      "Epoch 61 Loss: 2.9066\n",
      "Epoch 62 Loss: 2.8706\n",
      "Epoch 63 Loss: 2.8772\n",
      "Epoch 64 Loss: 2.8517\n",
      "Epoch 65 Loss: 2.8404\n",
      "Epoch 66 Loss: 2.8353\n",
      "Epoch 67 Loss: 2.8249\n",
      "Epoch 68 Loss: 2.8109\n",
      "Epoch 69 Loss: 2.8014\n",
      "Epoch 70 Loss: 2.7795\n",
      "Epoch 71 Loss: 2.7749\n",
      "Epoch 72 Loss: 2.7389\n",
      "Epoch 73 Loss: 2.7237\n",
      "Epoch 74 Loss: 2.7313\n",
      "Epoch 75 Loss: 2.7295\n",
      "Epoch 76 Loss: 2.6915\n",
      "Epoch 77 Loss: 2.6728\n",
      "Epoch 78 Loss: 2.6722\n",
      "Epoch 79 Loss: 2.6617\n",
      "Epoch 80 Loss: 2.6375\n",
      "Epoch 81 Loss: 2.6219\n",
      "Epoch 82 Loss: 2.6070\n",
      "Epoch 83 Loss: 2.6162\n",
      "Epoch 84 Loss: 2.6001\n",
      "Epoch 85 Loss: 2.5900\n",
      "Epoch 86 Loss: 2.5755\n",
      "Epoch 87 Loss: 2.5625\n",
      "Epoch 88 Loss: 2.5451\n",
      "Epoch 89 Loss: 2.5406\n",
      "Epoch 90 Loss: 2.5376\n",
      "Epoch 91 Loss: 2.5222\n",
      "Epoch 92 Loss: 2.5058\n",
      "Epoch 93 Loss: 2.5016\n",
      "Epoch 94 Loss: 2.4867\n",
      "Epoch 95 Loss: 2.4631\n",
      "Epoch 96 Loss: 2.4658\n",
      "Epoch 97 Loss: 2.4607\n",
      "Epoch 98 Loss: 2.4372\n",
      "Epoch 99 Loss: 2.4292\n",
      "Epoch 100 Loss: 2.4189\n",
      "Epoch 101 Loss: 2.3940\n",
      "Epoch 102 Loss: 2.4103\n",
      "Epoch 103 Loss: 2.3838\n",
      "Epoch 104 Loss: 2.3616\n",
      "Epoch 105 Loss: 2.3626\n",
      "Epoch 106 Loss: 2.3592\n",
      "Epoch 107 Loss: 2.3541\n",
      "Epoch 108 Loss: 2.3452\n",
      "Epoch 109 Loss: 2.3196\n",
      "Epoch 110 Loss: 2.3048\n",
      "Epoch 111 Loss: 2.3104\n",
      "Epoch 112 Loss: 2.2946\n",
      "Epoch 113 Loss: 2.2933\n",
      "Epoch 114 Loss: 2.2938\n",
      "Epoch 115 Loss: 2.2613\n",
      "Epoch 116 Loss: 2.2644\n",
      "Epoch 117 Loss: 2.2711\n",
      "Epoch 118 Loss: 2.2696\n",
      "Epoch 119 Loss: 2.2280\n",
      "Epoch 120 Loss: 2.2284\n",
      "Epoch 121 Loss: 2.2309\n",
      "Epoch 122 Loss: 2.2401\n",
      "Epoch 123 Loss: 2.2089\n",
      "Epoch 124 Loss: 2.1960\n",
      "Epoch 125 Loss: 2.1926\n",
      "Epoch 126 Loss: 2.1691\n",
      "Epoch 127 Loss: 2.1890\n",
      "Epoch 128 Loss: 2.1810\n",
      "Epoch 129 Loss: 2.1685\n",
      "Epoch 130 Loss: 2.1593\n",
      "Epoch 131 Loss: 2.1441\n",
      "Epoch 132 Loss: 2.1331\n",
      "Epoch 133 Loss: 2.1246\n",
      "Epoch 134 Loss: 2.1314\n",
      "Epoch 135 Loss: 2.1105\n",
      "Epoch 136 Loss: 2.1072\n",
      "Epoch 137 Loss: 2.1097\n",
      "Epoch 138 Loss: 2.0884\n",
      "Epoch 139 Loss: 2.0947\n",
      "Epoch 140 Loss: 2.0883\n",
      "Epoch 141 Loss: 2.0612\n",
      "Epoch 142 Loss: 2.0547\n",
      "Epoch 143 Loss: 2.0488\n",
      "Epoch 144 Loss: 2.0543\n",
      "Epoch 145 Loss: 2.0398\n",
      "Epoch 146 Loss: 2.0327\n",
      "Epoch 147 Loss: 2.0228\n",
      "Epoch 148 Loss: 2.0200\n",
      "Epoch 149 Loss: 2.0069\n",
      "Epoch 150 Loss: 2.0048\n",
      "Epoch 151 Loss: 1.9939\n",
      "Epoch 152 Loss: 2.0041\n",
      "Epoch 153 Loss: 1.9764\n",
      "Epoch 154 Loss: 1.9881\n",
      "Epoch 155 Loss: 1.9764\n",
      "Epoch 156 Loss: 1.9731\n",
      "Epoch 157 Loss: 1.9609\n",
      "Epoch 158 Loss: 1.9436\n",
      "Epoch 159 Loss: 1.9469\n",
      "Epoch 160 Loss: 1.9377\n",
      "Epoch 161 Loss: 1.9308\n",
      "Epoch 162 Loss: 1.9266\n",
      "Epoch 163 Loss: 1.9220\n",
      "Epoch 164 Loss: 1.9018\n",
      "Epoch 165 Loss: 1.9103\n",
      "Epoch 166 Loss: 1.9050\n",
      "Epoch 167 Loss: 1.9053\n",
      "Epoch 168 Loss: 1.8887\n",
      "Epoch 169 Loss: 1.8791\n",
      "Epoch 170 Loss: 1.8823\n",
      "Epoch 171 Loss: 1.8648\n",
      "Epoch 172 Loss: 1.8576\n",
      "Epoch 173 Loss: 1.8487\n",
      "Epoch 174 Loss: 1.8574\n",
      "Epoch 175 Loss: 1.8350\n",
      "Epoch 176 Loss: 1.8351\n",
      "Epoch 177 Loss: 1.8326\n",
      "Epoch 178 Loss: 1.8276\n",
      "Epoch 179 Loss: 1.8330\n",
      "Epoch 180 Loss: 1.8150\n",
      "Epoch 181 Loss: 1.7887\n",
      "Epoch 182 Loss: 1.8079\n",
      "Epoch 183 Loss: 1.8022\n",
      "Epoch 184 Loss: 1.7839\n",
      "Epoch 185 Loss: 1.7810\n",
      "Epoch 186 Loss: 1.7811\n",
      "Epoch 187 Loss: 1.7583\n",
      "Epoch 188 Loss: 1.7673\n",
      "Epoch 189 Loss: 1.7550\n",
      "Epoch 190 Loss: 1.7443\n",
      "Epoch 191 Loss: 1.7435\n",
      "Epoch 192 Loss: 1.7450\n",
      "Epoch 193 Loss: 1.7256\n",
      "Epoch 194 Loss: 1.7252\n",
      "Epoch 195 Loss: 1.7086\n",
      "Epoch 196 Loss: 1.7174\n",
      "Epoch 197 Loss: 1.7141\n",
      "Epoch 198 Loss: 1.7086\n",
      "Epoch 199 Loss: 1.6915\n",
      "Epoch 200 Loss: 1.6925\n",
      "Epoch 201 Loss: 1.6782\n",
      "Epoch 202 Loss: 1.6824\n",
      "Epoch 203 Loss: 1.6775\n",
      "Epoch 204 Loss: 1.6673\n",
      "Epoch 205 Loss: 1.6561\n",
      "Epoch 206 Loss: 1.6617\n",
      "Epoch 207 Loss: 1.6578\n",
      "Epoch 208 Loss: 1.6388\n",
      "Epoch 209 Loss: 1.6341\n",
      "Epoch 210 Loss: 1.6356\n",
      "Epoch 211 Loss: 1.6226\n",
      "Epoch 212 Loss: 1.6260\n",
      "Epoch 213 Loss: 1.6180\n",
      "Epoch 214 Loss: 1.6096\n",
      "Epoch 215 Loss: 1.6106\n",
      "Epoch 216 Loss: 1.5956\n",
      "Epoch 217 Loss: 1.5870\n",
      "Epoch 218 Loss: 1.5885\n",
      "Epoch 219 Loss: 1.5896\n",
      "Epoch 220 Loss: 1.5905\n",
      "Epoch 221 Loss: 1.5766\n",
      "Epoch 222 Loss: 1.5558\n",
      "Epoch 223 Loss: 1.5646\n",
      "Epoch 224 Loss: 1.5631\n",
      "Epoch 225 Loss: 1.5518\n",
      "Epoch 226 Loss: 1.5511\n",
      "Epoch 227 Loss: 1.5290\n",
      "Epoch 228 Loss: 1.5401\n",
      "Epoch 229 Loss: 1.5287\n",
      "Epoch 230 Loss: 1.5208\n",
      "Epoch 231 Loss: 1.5176\n",
      "Epoch 232 Loss: 1.5177\n",
      "Epoch 233 Loss: 1.5007\n",
      "Epoch 234 Loss: 1.4903\n",
      "Epoch 235 Loss: 1.4948\n",
      "Epoch 236 Loss: 1.4799\n",
      "Epoch 237 Loss: 1.5044\n",
      "Epoch 238 Loss: 1.4857\n",
      "Epoch 239 Loss: 1.4740\n",
      "Epoch 240 Loss: 1.4696\n",
      "Epoch 241 Loss: 1.4691\n",
      "Epoch 242 Loss: 1.4590\n",
      "Epoch 243 Loss: 1.4604\n",
      "Epoch 244 Loss: 1.4445\n",
      "Epoch 245 Loss: 1.4556\n",
      "Epoch 246 Loss: 1.4341\n",
      "Epoch 247 Loss: 1.4213\n",
      "Epoch 248 Loss: 1.4227\n",
      "Epoch 249 Loss: 1.4129\n",
      "Epoch 250 Loss: 1.4179\n",
      "Epoch 251 Loss: 1.4099\n",
      "Epoch 252 Loss: 1.4174\n",
      "Epoch 253 Loss: 1.4066\n",
      "Epoch 254 Loss: 1.4008\n",
      "Epoch 255 Loss: 1.3906\n",
      "Epoch 256 Loss: 1.3921\n",
      "Epoch 257 Loss: 1.3967\n",
      "Epoch 258 Loss: 1.3660\n",
      "Epoch 259 Loss: 1.3807\n",
      "Epoch 260 Loss: 1.3697\n",
      "Epoch 261 Loss: 1.3629\n",
      "Epoch 262 Loss: 1.3713\n",
      "Epoch 263 Loss: 1.3642\n",
      "Epoch 264 Loss: 1.3390\n",
      "Epoch 265 Loss: 1.3523\n",
      "Epoch 266 Loss: 1.3254\n",
      "Epoch 267 Loss: 1.3294\n",
      "Epoch 268 Loss: 1.3406\n",
      "Epoch 269 Loss: 1.3270\n",
      "Epoch 270 Loss: 1.3221\n",
      "Epoch 271 Loss: 1.3093\n",
      "Epoch 272 Loss: 1.3178\n",
      "Epoch 273 Loss: 1.3145\n",
      "Epoch 274 Loss: 1.3168\n",
      "Epoch 275 Loss: 1.3228\n",
      "Epoch 276 Loss: 1.2958\n",
      "Epoch 277 Loss: 1.2865\n",
      "Epoch 278 Loss: 1.2716\n",
      "Epoch 279 Loss: 1.2672\n",
      "Epoch 280 Loss: 1.2752\n",
      "Epoch 281 Loss: 1.2776\n",
      "Epoch 282 Loss: 1.2722\n",
      "Epoch 283 Loss: 1.2725\n",
      "Epoch 284 Loss: 1.2754\n",
      "Epoch 285 Loss: 1.2604\n",
      "Epoch 286 Loss: 1.2478\n",
      "Epoch 287 Loss: 1.2594\n",
      "Epoch 288 Loss: 1.2396\n",
      "Epoch 289 Loss: 1.2453\n",
      "Epoch 290 Loss: 1.2413\n",
      "Epoch 291 Loss: 1.2319\n",
      "Epoch 292 Loss: 1.2258\n",
      "Epoch 293 Loss: 1.2248\n",
      "Epoch 294 Loss: 1.2205\n",
      "Epoch 295 Loss: 1.2298\n",
      "Epoch 296 Loss: 1.2072\n",
      "Epoch 297 Loss: 1.2137\n",
      "Epoch 298 Loss: 1.2140\n",
      "Epoch 299 Loss: 1.1983\n",
      "Epoch 300 Loss: 1.1857\n",
      "Epoch 301 Loss: 1.1761\n",
      "Epoch 302 Loss: 1.1945\n",
      "Epoch 303 Loss: 1.1891\n",
      "Epoch 304 Loss: 1.1866\n",
      "Epoch 305 Loss: 1.1897\n",
      "Epoch 306 Loss: 1.1784\n",
      "Epoch 307 Loss: 1.1728\n",
      "Epoch 308 Loss: 1.1643\n",
      "Epoch 309 Loss: 1.1610\n",
      "Epoch 310 Loss: 1.1503\n",
      "Epoch 311 Loss: 1.1392\n",
      "Epoch 312 Loss: 1.1446\n",
      "Epoch 313 Loss: 1.1433\n",
      "Epoch 314 Loss: 1.1482\n",
      "Epoch 315 Loss: 1.1668\n",
      "Epoch 316 Loss: 1.1361\n",
      "Epoch 317 Loss: 1.1315\n",
      "Epoch 318 Loss: 1.1287\n",
      "Epoch 319 Loss: 1.1341\n",
      "Epoch 320 Loss: 1.1136\n",
      "Epoch 321 Loss: 1.1045\n",
      "Epoch 322 Loss: 1.1155\n",
      "Epoch 323 Loss: 1.1327\n",
      "Epoch 324 Loss: 1.1201\n",
      "Epoch 325 Loss: 1.1033\n",
      "Epoch 326 Loss: 1.0943\n",
      "Epoch 327 Loss: 1.1131\n",
      "Epoch 328 Loss: 1.0981\n",
      "Epoch 329 Loss: 1.0775\n",
      "Epoch 330 Loss: 1.0840\n",
      "Epoch 331 Loss: 1.0835\n",
      "Epoch 332 Loss: 1.0648\n",
      "Epoch 333 Loss: 1.0949\n",
      "Epoch 334 Loss: 1.0607\n",
      "Epoch 335 Loss: 1.0636\n",
      "Epoch 336 Loss: 1.0637\n",
      "Epoch 337 Loss: 1.0619\n",
      "Epoch 338 Loss: 1.0435\n",
      "Epoch 339 Loss: 1.0568\n",
      "Epoch 340 Loss: 1.0405\n",
      "Epoch 341 Loss: 1.0380\n",
      "Epoch 342 Loss: 1.0526\n",
      "Epoch 343 Loss: 1.0513\n",
      "Epoch 344 Loss: 1.0404\n",
      "Epoch 345 Loss: 1.0377\n",
      "Epoch 346 Loss: 1.0346\n",
      "Epoch 347 Loss: 1.0251\n",
      "Epoch 348 Loss: 1.0058\n",
      "Epoch 349 Loss: 1.0170\n",
      "Epoch 350 Loss: 1.0125\n",
      "Epoch 351 Loss: 1.0200\n",
      "Epoch 352 Loss: 1.0033\n",
      "Epoch 353 Loss: 0.9985\n",
      "Epoch 354 Loss: 1.0107\n",
      "Epoch 355 Loss: 1.0135\n",
      "Epoch 356 Loss: 1.0013\n",
      "Epoch 357 Loss: 1.0072\n",
      "Epoch 358 Loss: 0.9824\n",
      "Epoch 359 Loss: 0.9924\n",
      "Epoch 360 Loss: 0.9737\n",
      "Epoch 361 Loss: 0.9784\n",
      "Epoch 362 Loss: 0.9777\n",
      "Epoch 363 Loss: 0.9802\n",
      "Epoch 364 Loss: 0.9655\n",
      "Epoch 365 Loss: 0.9643\n",
      "Epoch 366 Loss: 0.9593\n",
      "Epoch 367 Loss: 0.9579\n",
      "Epoch 368 Loss: 0.9498\n",
      "Epoch 369 Loss: 0.9349\n",
      "Epoch 370 Loss: 0.9585\n",
      "Epoch 371 Loss: 0.9284\n",
      "Epoch 372 Loss: 0.9410\n",
      "Epoch 373 Loss: 0.9405\n",
      "Epoch 374 Loss: 0.9422\n",
      "Epoch 375 Loss: 0.9527\n",
      "Epoch 376 Loss: 0.9225\n",
      "Epoch 377 Loss: 0.9237\n",
      "Epoch 378 Loss: 0.9132\n",
      "Epoch 379 Loss: 0.9193\n",
      "Epoch 380 Loss: 0.9024\n",
      "Epoch 381 Loss: 0.9220\n",
      "Epoch 382 Loss: 0.9240\n",
      "Epoch 383 Loss: 0.9125\n",
      "Epoch 384 Loss: 0.9089\n",
      "Epoch 385 Loss: 0.9087\n",
      "Epoch 386 Loss: 0.8909\n",
      "Epoch 387 Loss: 0.8811\n",
      "Epoch 388 Loss: 0.8735\n",
      "Epoch 389 Loss: 0.8902\n",
      "Epoch 390 Loss: 0.8876\n",
      "Epoch 391 Loss: 0.8850\n",
      "Epoch 392 Loss: 0.8986\n",
      "Epoch 393 Loss: 0.8770\n",
      "Epoch 394 Loss: 0.8685\n",
      "Epoch 395 Loss: 0.8713\n",
      "Epoch 396 Loss: 0.8793\n",
      "Epoch 397 Loss: 0.8468\n",
      "Epoch 398 Loss: 0.8698\n",
      "Epoch 399 Loss: 0.8726\n",
      "Epoch 400 Loss: 0.8518\n",
      "Epoch 401 Loss: 0.8643\n",
      "Epoch 402 Loss: 0.8360\n",
      "Epoch 403 Loss: 0.8635\n",
      "Epoch 404 Loss: 0.8367\n",
      "Epoch 405 Loss: 0.8371\n",
      "Epoch 406 Loss: 0.8460\n",
      "Epoch 407 Loss: 0.8491\n",
      "Epoch 408 Loss: 0.8423\n",
      "Epoch 409 Loss: 0.8413\n",
      "Epoch 410 Loss: 0.8267\n",
      "Epoch 411 Loss: 0.8239\n",
      "Epoch 412 Loss: 0.8240\n",
      "Epoch 413 Loss: 0.8252\n",
      "Epoch 414 Loss: 0.8089\n",
      "Epoch 415 Loss: 0.8226\n",
      "Epoch 416 Loss: 0.8208\n",
      "Epoch 417 Loss: 0.8027\n",
      "Epoch 418 Loss: 0.8025\n",
      "Epoch 419 Loss: 0.7960\n",
      "Epoch 420 Loss: 0.7928\n",
      "Epoch 421 Loss: 0.8146\n",
      "Epoch 422 Loss: 0.7805\n",
      "Epoch 423 Loss: 0.7935\n",
      "Epoch 424 Loss: 0.8024\n",
      "Epoch 425 Loss: 0.7923\n",
      "Epoch 426 Loss: 0.7873\n",
      "Epoch 427 Loss: 0.7792\n",
      "Epoch 428 Loss: 0.7716\n",
      "Epoch 429 Loss: 0.7946\n",
      "Epoch 430 Loss: 0.7883\n",
      "Epoch 431 Loss: 0.7741\n",
      "Epoch 432 Loss: 0.7816\n",
      "Epoch 433 Loss: 0.7600\n",
      "Epoch 434 Loss: 0.7662\n",
      "Epoch 435 Loss: 0.7611\n",
      "Epoch 436 Loss: 0.7387\n",
      "Epoch 437 Loss: 0.7528\n",
      "Epoch 438 Loss: 0.7364\n",
      "Epoch 439 Loss: 0.7532\n",
      "Epoch 440 Loss: 0.7488\n",
      "Epoch 441 Loss: 0.7625\n",
      "Epoch 442 Loss: 0.7402\n",
      "Epoch 443 Loss: 0.7415\n",
      "Epoch 444 Loss: 0.7483\n",
      "Epoch 445 Loss: 0.7335\n",
      "Epoch 446 Loss: 0.7126\n",
      "Epoch 447 Loss: 0.7295\n",
      "Epoch 448 Loss: 0.7275\n",
      "Epoch 449 Loss: 0.7322\n",
      "Epoch 450 Loss: 0.7193\n",
      "Epoch 451 Loss: 0.7123\n",
      "Epoch 452 Loss: 0.7172\n",
      "Epoch 453 Loss: 0.7054\n",
      "Epoch 454 Loss: 0.7267\n",
      "Epoch 455 Loss: 0.7027\n",
      "Epoch 456 Loss: 0.7268\n",
      "Epoch 457 Loss: 0.7035\n",
      "Epoch 458 Loss: 0.7147\n",
      "Epoch 459 Loss: 0.6919\n",
      "Epoch 460 Loss: 0.7220\n",
      "Epoch 461 Loss: 0.7112\n",
      "Epoch 462 Loss: 0.6734\n",
      "Epoch 463 Loss: 0.6931\n",
      "Epoch 464 Loss: 0.7013\n",
      "Epoch 465 Loss: 0.6832\n",
      "Epoch 466 Loss: 0.6629\n",
      "Epoch 467 Loss: 0.6726\n",
      "Epoch 468 Loss: 0.6784\n",
      "Epoch 469 Loss: 0.6647\n",
      "Epoch 470 Loss: 0.6695\n",
      "Epoch 471 Loss: 0.6927\n",
      "Epoch 472 Loss: 0.6622\n",
      "Epoch 473 Loss: 0.6713\n",
      "Epoch 474 Loss: 0.6635\n",
      "Epoch 475 Loss: 0.6427\n",
      "Epoch 476 Loss: 0.6789\n",
      "Epoch 477 Loss: 0.6464\n",
      "Epoch 478 Loss: 0.6600\n",
      "Epoch 479 Loss: 0.6400\n",
      "Epoch 480 Loss: 0.6555\n",
      "Epoch 481 Loss: 0.6255\n",
      "Epoch 482 Loss: 0.6411\n",
      "Epoch 483 Loss: 0.6381\n",
      "Epoch 484 Loss: 0.6329\n",
      "Epoch 485 Loss: 0.6295\n",
      "Epoch 486 Loss: 0.6462\n",
      "Epoch 487 Loss: 0.6256\n",
      "Epoch 488 Loss: 0.6167\n",
      "Epoch 489 Loss: 0.6168\n",
      "Epoch 490 Loss: 0.6203\n",
      "Epoch 491 Loss: 0.6220\n",
      "Epoch 492 Loss: 0.6358\n",
      "Epoch 493 Loss: 0.6065\n",
      "Epoch 494 Loss: 0.6350\n",
      "Epoch 495 Loss: 0.6131\n",
      "Epoch 496 Loss: 0.5949\n",
      "Epoch 497 Loss: 0.6044\n",
      "Epoch 498 Loss: 0.5950\n",
      "Epoch 499 Loss: 0.5961\n",
      "Epoch 500 Loss: 0.5926\n",
      "Epoch 501 Loss: 0.5971\n",
      "Epoch 502 Loss: 0.5961\n",
      "Epoch 503 Loss: 0.5804\n",
      "Epoch 504 Loss: 0.5964\n",
      "Epoch 505 Loss: 0.6002\n",
      "Epoch 506 Loss: 0.5933\n",
      "Epoch 507 Loss: 0.5884\n",
      "Epoch 508 Loss: 0.6008\n",
      "Epoch 509 Loss: 0.5952\n",
      "Epoch 510 Loss: 0.5591\n",
      "Epoch 511 Loss: 0.5883\n",
      "Epoch 512 Loss: 0.5782\n",
      "Epoch 513 Loss: 0.5550\n",
      "Epoch 514 Loss: 0.5718\n",
      "Epoch 515 Loss: 0.5599\n",
      "Epoch 516 Loss: 0.5660\n",
      "Epoch 517 Loss: 0.5778\n",
      "Epoch 518 Loss: 0.5608\n",
      "Epoch 519 Loss: 0.5555\n",
      "Epoch 520 Loss: 0.5513\n",
      "Epoch 521 Loss: 0.5587\n",
      "Epoch 522 Loss: 0.5350\n",
      "Epoch 523 Loss: 0.5609\n",
      "Epoch 524 Loss: 0.5263\n",
      "Epoch 525 Loss: 0.5407\n",
      "Epoch 526 Loss: 0.5583\n",
      "Epoch 527 Loss: 0.5317\n",
      "Epoch 528 Loss: 0.5622\n",
      "Epoch 529 Loss: 0.5454\n",
      "Epoch 530 Loss: 0.5493\n",
      "Epoch 531 Loss: 0.5461\n",
      "Epoch 532 Loss: 0.5146\n",
      "Epoch 533 Loss: 0.5602\n",
      "Epoch 534 Loss: 0.5540\n",
      "Epoch 535 Loss: 0.5270\n",
      "Epoch 536 Loss: 0.5439\n",
      "Epoch 537 Loss: 0.5157\n",
      "Epoch 538 Loss: 0.5094\n",
      "Epoch 539 Loss: 0.5044\n",
      "Epoch 540 Loss: 0.5198\n",
      "Epoch 541 Loss: 0.5009\n",
      "Epoch 542 Loss: 0.5316\n",
      "Epoch 543 Loss: 0.5021\n",
      "Epoch 544 Loss: 0.5348\n",
      "Epoch 545 Loss: 0.4982\n",
      "Epoch 546 Loss: 0.5054\n",
      "Epoch 547 Loss: 0.5002\n",
      "Epoch 548 Loss: 0.5289\n",
      "Epoch 549 Loss: 0.4991\n",
      "Epoch 550 Loss: 0.5102\n",
      "Epoch 551 Loss: 0.5183\n",
      "Epoch 552 Loss: 0.5095\n",
      "Epoch 553 Loss: 0.5028\n",
      "Epoch 554 Loss: 0.4946\n",
      "Epoch 555 Loss: 0.4748\n",
      "Epoch 556 Loss: 0.4661\n",
      "Epoch 557 Loss: 0.4947\n",
      "Epoch 558 Loss: 0.4879\n",
      "Epoch 559 Loss: 0.5007\n",
      "Epoch 560 Loss: 0.5033\n",
      "Epoch 561 Loss: 0.4815\n",
      "Epoch 562 Loss: 0.4605\n",
      "Epoch 563 Loss: 0.4748\n",
      "Epoch 564 Loss: 0.4812\n",
      "Epoch 565 Loss: 0.4850\n",
      "Epoch 566 Loss: 0.4630\n",
      "Epoch 567 Loss: 0.4684\n",
      "Epoch 568 Loss: 0.4733\n",
      "Epoch 569 Loss: 0.4899\n",
      "Epoch 570 Loss: 0.4732\n",
      "Epoch 571 Loss: 0.4594\n",
      "Epoch 572 Loss: 0.4604\n",
      "Epoch 573 Loss: 0.4576\n",
      "Epoch 574 Loss: 0.4830\n",
      "Epoch 575 Loss: 0.4443\n",
      "Epoch 576 Loss: 0.4658\n",
      "Epoch 577 Loss: 0.4358\n",
      "Epoch 578 Loss: 0.4434\n",
      "Epoch 579 Loss: 0.4544\n",
      "Epoch 580 Loss: 0.4402\n",
      "Epoch 581 Loss: 0.4417\n",
      "Epoch 582 Loss: 0.4544\n",
      "Epoch 583 Loss: 0.4657\n",
      "Epoch 584 Loss: 0.4277\n",
      "Epoch 585 Loss: 0.4713\n",
      "Epoch 586 Loss: 0.4587\n",
      "Epoch 587 Loss: 0.4391\n",
      "Epoch 588 Loss: 0.4358\n",
      "Epoch 589 Loss: 0.4730\n",
      "Epoch 590 Loss: 0.4427\n",
      "Epoch 591 Loss: 0.4362\n",
      "Epoch 592 Loss: 0.4367\n",
      "Epoch 593 Loss: 0.4073\n",
      "Epoch 594 Loss: 0.4284\n",
      "Epoch 595 Loss: 0.4444\n",
      "Epoch 596 Loss: 0.4236\n",
      "Epoch 597 Loss: 0.4136\n",
      "Epoch 598 Loss: 0.4548\n",
      "Epoch 599 Loss: 0.4391\n",
      "Epoch 600 Loss: 0.4605\n",
      "Epoch 601 Loss: 0.3916\n",
      "Epoch 602 Loss: 0.4482\n",
      "Epoch 603 Loss: 0.4330\n",
      "Epoch 604 Loss: 0.4148\n",
      "Epoch 605 Loss: 0.4308\n",
      "Epoch 606 Loss: 0.4278\n",
      "Epoch 607 Loss: 0.4108\n",
      "Epoch 608 Loss: 0.4148\n",
      "Epoch 609 Loss: 0.4500\n",
      "Epoch 610 Loss: 0.3905\n",
      "Epoch 611 Loss: 0.4270\n",
      "Epoch 612 Loss: 0.4064\n",
      "Epoch 613 Loss: 0.4106\n",
      "Epoch 614 Loss: 0.4348\n",
      "Epoch 615 Loss: 0.4048\n",
      "Epoch 616 Loss: 0.4284\n",
      "Epoch 617 Loss: 0.3928\n",
      "Epoch 618 Loss: 0.3865\n",
      "Epoch 619 Loss: 0.3684\n",
      "Epoch 620 Loss: 0.3800\n",
      "Epoch 621 Loss: 0.4369\n",
      "Epoch 622 Loss: 0.4015\n",
      "Epoch 623 Loss: 0.3947\n",
      "Epoch 624 Loss: 0.3889\n",
      "Epoch 625 Loss: 0.3670\n",
      "Epoch 626 Loss: 0.4058\n",
      "Epoch 627 Loss: 0.3826\n",
      "Epoch 628 Loss: 0.3826\n",
      "Epoch 629 Loss: 0.3929\n",
      "Epoch 630 Loss: 0.3820\n",
      "Epoch 631 Loss: 0.3944\n",
      "Epoch 632 Loss: 0.3862\n",
      "Epoch 633 Loss: 0.3914\n",
      "Epoch 634 Loss: 0.3960\n",
      "Epoch 635 Loss: 0.3961\n",
      "Epoch 636 Loss: 0.3879\n",
      "Epoch 637 Loss: 0.3976\n",
      "Epoch 638 Loss: 0.3971\n",
      "Epoch 639 Loss: 0.3667\n",
      "Epoch 640 Loss: 0.3529\n",
      "Epoch 641 Loss: 0.3863\n",
      "Epoch 642 Loss: 0.3732\n",
      "Epoch 643 Loss: 0.3663\n",
      "Epoch 644 Loss: 0.3474\n",
      "Epoch 645 Loss: 0.3782\n",
      "Epoch 646 Loss: 0.3607\n",
      "Epoch 647 Loss: 0.3629\n",
      "Epoch 648 Loss: 0.3224\n",
      "Epoch 649 Loss: 0.3796\n",
      "Epoch 650 Loss: 0.3880\n",
      "Epoch 651 Loss: 0.3552\n",
      "Epoch 652 Loss: 0.3531\n",
      "Epoch 653 Loss: 0.3675\n",
      "Epoch 654 Loss: 0.3350\n",
      "Epoch 655 Loss: 0.3311\n",
      "Epoch 656 Loss: 0.3671\n",
      "Epoch 657 Loss: 0.3394\n",
      "Epoch 658 Loss: 0.3405\n",
      "Epoch 659 Loss: 0.3517\n",
      "Epoch 660 Loss: 0.3418\n",
      "Epoch 661 Loss: 0.3268\n",
      "Epoch 662 Loss: 0.3258\n",
      "Epoch 663 Loss: 0.3451\n",
      "Epoch 664 Loss: 0.3342\n",
      "Epoch 665 Loss: 0.3503\n",
      "Epoch 666 Loss: 0.3431\n",
      "Epoch 667 Loss: 0.3359\n",
      "Epoch 668 Loss: 0.3494\n",
      "Epoch 669 Loss: 0.3300\n",
      "Epoch 670 Loss: 0.3171\n",
      "Epoch 671 Loss: 0.3328\n",
      "Epoch 672 Loss: 0.3305\n",
      "Epoch 673 Loss: 0.3366\n",
      "Epoch 674 Loss: 0.3476\n",
      "Epoch 675 Loss: 0.3195\n",
      "Epoch 676 Loss: 0.3134\n",
      "Epoch 677 Loss: 0.3698\n",
      "Epoch 678 Loss: 0.3114\n",
      "Epoch 679 Loss: 0.3217\n",
      "Epoch 680 Loss: 0.3090\n",
      "Epoch 681 Loss: 0.3176\n",
      "Epoch 682 Loss: 0.3214\n",
      "Epoch 683 Loss: 0.3330\n",
      "Epoch 684 Loss: 0.3154\n",
      "Epoch 685 Loss: 0.3218\n",
      "Epoch 686 Loss: 0.2989\n",
      "Epoch 687 Loss: 0.3199\n",
      "Epoch 688 Loss: 0.3091\n",
      "Epoch 689 Loss: 0.3173\n",
      "Epoch 690 Loss: 0.3331\n",
      "Epoch 691 Loss: 0.3091\n",
      "Epoch 692 Loss: 0.3065\n",
      "Epoch 693 Loss: 0.3270\n",
      "Epoch 694 Loss: 0.3256\n",
      "Epoch 695 Loss: 0.3193\n",
      "Epoch 696 Loss: 0.3114\n",
      "Epoch 697 Loss: 0.3281\n",
      "Epoch 698 Loss: 0.3161\n",
      "Epoch 699 Loss: 0.3081\n",
      "Epoch 700 Loss: 0.3042\n",
      "Epoch 701 Loss: 0.2983\n",
      "Epoch 702 Loss: 0.3041\n",
      "Epoch 703 Loss: 0.3161\n",
      "Epoch 704 Loss: 0.3049\n",
      "Epoch 705 Loss: 0.2988\n",
      "Epoch 706 Loss: 0.2808\n",
      "Epoch 707 Loss: 0.3032\n",
      "Epoch 708 Loss: 0.3014\n",
      "Epoch 709 Loss: 0.2906\n",
      "Epoch 710 Loss: 0.2917\n",
      "Epoch 711 Loss: 0.3135\n",
      "Epoch 712 Loss: 0.3082\n",
      "Epoch 713 Loss: 0.2612\n",
      "Epoch 714 Loss: 0.2770\n",
      "Epoch 715 Loss: 0.2962\n",
      "Epoch 716 Loss: 0.2872\n",
      "Epoch 717 Loss: 0.2873\n",
      "Epoch 718 Loss: 0.2740\n",
      "Epoch 719 Loss: 0.2942\n",
      "Epoch 720 Loss: 0.3110\n",
      "Epoch 721 Loss: 0.2977\n",
      "Epoch 722 Loss: 0.2677\n",
      "Epoch 723 Loss: 0.2572\n",
      "Epoch 724 Loss: 0.2727\n",
      "Epoch 725 Loss: 0.2655\n",
      "Epoch 726 Loss: 0.2946\n",
      "Epoch 727 Loss: 0.2698\n",
      "Epoch 728 Loss: 0.2716\n",
      "Epoch 729 Loss: 0.3020\n",
      "Epoch 730 Loss: 0.2484\n",
      "Epoch 731 Loss: 0.2771\n",
      "Epoch 732 Loss: 0.3057\n",
      "Epoch 733 Loss: 0.2717\n",
      "Epoch 734 Loss: 0.2718\n",
      "Epoch 735 Loss: 0.3136\n",
      "Epoch 736 Loss: 0.2829\n",
      "Epoch 737 Loss: 0.2708\n",
      "Epoch 738 Loss: 0.2807\n",
      "Epoch 739 Loss: 0.2571\n",
      "Epoch 740 Loss: 0.2743\n",
      "Epoch 741 Loss: 0.2738\n",
      "Epoch 742 Loss: 0.2830\n",
      "Epoch 743 Loss: 0.2609\n",
      "Epoch 744 Loss: 0.2876\n",
      "Epoch 745 Loss: 0.2922\n",
      "Epoch 746 Loss: 0.2479\n",
      "Epoch 747 Loss: 0.2693\n",
      "Epoch 748 Loss: 0.2867\n",
      "Epoch 749 Loss: 0.2582\n",
      "Epoch 750 Loss: 0.2624\n",
      "Epoch 751 Loss: 0.2750\n",
      "Epoch 752 Loss: 0.2598\n",
      "Epoch 753 Loss: 0.2584\n",
      "Epoch 754 Loss: 0.2478\n",
      "Epoch 755 Loss: 0.2753\n",
      "Epoch 756 Loss: 0.2766\n",
      "Epoch 757 Loss: 0.2716\n",
      "Epoch 758 Loss: 0.2474\n",
      "Epoch 759 Loss: 0.2524\n",
      "Epoch 760 Loss: 0.2677\n",
      "Epoch 761 Loss: 0.2515\n",
      "Epoch 762 Loss: 0.2359\n",
      "Epoch 763 Loss: 0.2740\n",
      "Epoch 764 Loss: 0.3005\n",
      "Epoch 765 Loss: 0.2519\n",
      "Epoch 766 Loss: 0.2512\n",
      "Epoch 767 Loss: 0.2416\n",
      "Epoch 768 Loss: 0.2641\n",
      "Epoch 769 Loss: 0.2591\n",
      "Epoch 770 Loss: 0.2437\n",
      "Epoch 771 Loss: 0.2666\n",
      "Epoch 772 Loss: 0.2507\n",
      "Epoch 773 Loss: 0.2542\n",
      "Epoch 774 Loss: 0.2518\n",
      "Epoch 775 Loss: 0.2553\n",
      "Epoch 776 Loss: 0.2467\n",
      "Epoch 777 Loss: 0.2498\n",
      "Epoch 778 Loss: 0.2674\n",
      "Epoch 779 Loss: 0.2351\n",
      "Epoch 780 Loss: 0.2376\n",
      "Epoch 781 Loss: 0.2528\n",
      "Epoch 782 Loss: 0.2269\n",
      "Epoch 783 Loss: 0.2355\n",
      "Epoch 784 Loss: 0.2523\n",
      "Epoch 785 Loss: 0.2360\n",
      "Epoch 786 Loss: 0.2628\n",
      "Epoch 787 Loss: 0.2482\n",
      "Epoch 788 Loss: 0.2463\n",
      "Epoch 789 Loss: 0.2316\n",
      "Epoch 790 Loss: 0.2342\n",
      "Epoch 791 Loss: 0.2376\n",
      "Epoch 792 Loss: 0.2163\n",
      "Epoch 793 Loss: 0.2585\n",
      "Epoch 794 Loss: 0.2389\n",
      "Epoch 795 Loss: 0.2293\n",
      "Epoch 796 Loss: 0.2450\n",
      "Epoch 797 Loss: 0.2430\n",
      "Epoch 798 Loss: 0.2228\n",
      "Epoch 799 Loss: 0.2591\n",
      "Epoch 800 Loss: 0.2391\n",
      "Epoch 801 Loss: 0.2508\n",
      "Epoch 802 Loss: 0.2409\n",
      "Epoch 803 Loss: 0.2380\n",
      "Epoch 804 Loss: 0.2303\n",
      "Epoch 805 Loss: 0.2058\n",
      "Epoch 806 Loss: 0.2270\n",
      "Epoch 807 Loss: 0.2397\n",
      "Epoch 808 Loss: 0.2375\n",
      "Epoch 809 Loss: 0.2262\n",
      "Epoch 810 Loss: 0.2071\n",
      "Epoch 811 Loss: 0.2317\n",
      "Epoch 812 Loss: 0.2292\n",
      "Epoch 813 Loss: 0.2251\n",
      "Epoch 814 Loss: 0.2126\n",
      "Epoch 815 Loss: 0.2372\n",
      "Epoch 816 Loss: 0.2465\n",
      "Epoch 817 Loss: 0.2203\n",
      "Epoch 818 Loss: 0.2150\n",
      "Epoch 819 Loss: 0.2351\n",
      "Epoch 820 Loss: 0.2312\n",
      "Epoch 821 Loss: 0.2185\n",
      "Epoch 822 Loss: 0.2400\n",
      "Epoch 823 Loss: 0.2197\n",
      "Epoch 824 Loss: 0.2314\n",
      "Epoch 825 Loss: 0.2537\n",
      "Epoch 826 Loss: 0.2268\n",
      "Epoch 827 Loss: 0.2134\n",
      "Epoch 828 Loss: 0.2371\n",
      "Epoch 829 Loss: 0.2147\n",
      "Epoch 830 Loss: 0.2075\n",
      "Epoch 831 Loss: 0.2048\n",
      "Epoch 832 Loss: 0.2214\n",
      "Epoch 833 Loss: 0.2401\n",
      "Epoch 834 Loss: 0.2045\n",
      "Epoch 835 Loss: 0.2222\n",
      "Epoch 836 Loss: 0.2053\n",
      "Epoch 837 Loss: 0.2093\n",
      "Epoch 838 Loss: 0.2196\n",
      "Epoch 839 Loss: 0.2197\n",
      "Epoch 840 Loss: 0.2266\n",
      "Epoch 841 Loss: 0.1870\n",
      "Epoch 842 Loss: 0.1969\n",
      "Epoch 843 Loss: 0.2138\n",
      "Epoch 844 Loss: 0.2099\n",
      "Epoch 845 Loss: 0.2298\n",
      "Epoch 846 Loss: 0.2515\n",
      "Epoch 847 Loss: 0.2018\n",
      "Epoch 848 Loss: 0.1973\n",
      "Epoch 849 Loss: 0.2262\n",
      "Epoch 850 Loss: 0.2095\n",
      "Epoch 851 Loss: 0.2093\n",
      "Epoch 852 Loss: 0.2076\n",
      "Epoch 853 Loss: 0.2141\n",
      "Epoch 854 Loss: 0.2153\n",
      "Epoch 855 Loss: 0.2116\n",
      "Epoch 856 Loss: 0.2299\n",
      "Epoch 857 Loss: 0.2003\n",
      "Epoch 858 Loss: 0.2142\n",
      "Epoch 859 Loss: 0.2021\n",
      "Epoch 860 Loss: 0.1767\n",
      "Epoch 861 Loss: 0.2112\n",
      "Epoch 862 Loss: 0.2170\n",
      "Epoch 863 Loss: 0.1993\n",
      "Epoch 864 Loss: 0.1872\n",
      "Epoch 865 Loss: 0.1986\n",
      "Epoch 866 Loss: 0.1852\n",
      "Epoch 867 Loss: 0.2094\n",
      "Epoch 868 Loss: 0.2303\n",
      "Epoch 869 Loss: 0.1888\n",
      "Epoch 870 Loss: 0.1831\n",
      "Epoch 871 Loss: 0.2110\n",
      "Epoch 872 Loss: 0.1985\n",
      "Epoch 873 Loss: 0.1923\n",
      "Epoch 874 Loss: 0.1885\n",
      "Epoch 875 Loss: 0.2102\n",
      "Epoch 876 Loss: 0.1904\n",
      "Epoch 877 Loss: 0.1987\n",
      "Epoch 878 Loss: 0.1820\n",
      "Epoch 879 Loss: 0.1869\n",
      "Epoch 880 Loss: 0.1818\n",
      "Epoch 881 Loss: 0.1669\n",
      "Epoch 882 Loss: 0.2054\n",
      "Epoch 883 Loss: 0.1982\n",
      "Epoch 884 Loss: 0.1967\n",
      "Epoch 885 Loss: 0.1832\n",
      "Epoch 886 Loss: 0.2220\n",
      "Epoch 887 Loss: 0.1795\n",
      "Epoch 888 Loss: 0.1860\n",
      "Epoch 889 Loss: 0.1751\n",
      "Epoch 890 Loss: 0.1987\n",
      "Epoch 891 Loss: 0.1909\n",
      "Epoch 892 Loss: 0.1651\n",
      "Epoch 893 Loss: 0.1937\n",
      "Epoch 894 Loss: 0.2146\n",
      "Epoch 895 Loss: 0.1829\n",
      "Epoch 896 Loss: 0.1774\n",
      "Epoch 897 Loss: 0.1682\n",
      "Epoch 898 Loss: 0.1852\n",
      "Epoch 899 Loss: 0.1928\n",
      "Epoch 900 Loss: 0.1600\n",
      "Epoch 901 Loss: 0.1878\n",
      "Epoch 902 Loss: 0.1806\n",
      "Epoch 903 Loss: 0.1865\n",
      "Epoch 904 Loss: 0.1966\n",
      "Epoch 905 Loss: 0.1841\n",
      "Epoch 906 Loss: 0.1681\n",
      "Epoch 907 Loss: 0.1624\n",
      "Epoch 908 Loss: 0.1839\n",
      "Epoch 909 Loss: 0.1685\n",
      "Epoch 910 Loss: 0.1682\n",
      "Epoch 911 Loss: 0.1809\n",
      "Epoch 912 Loss: 0.1894\n",
      "Epoch 913 Loss: 0.1700\n",
      "Epoch 914 Loss: 0.1590\n",
      "Epoch 915 Loss: 0.1878\n",
      "Epoch 916 Loss: 0.1726\n",
      "Epoch 917 Loss: 0.1697\n",
      "Epoch 918 Loss: 0.1669\n",
      "Epoch 919 Loss: 0.1678\n",
      "Epoch 920 Loss: 0.1673\n",
      "Epoch 921 Loss: 0.1650\n",
      "Epoch 922 Loss: 0.1771\n",
      "Epoch 923 Loss: 0.1729\n",
      "Epoch 924 Loss: 0.1718\n",
      "Epoch 925 Loss: 0.1763\n",
      "Epoch 926 Loss: 0.1785\n",
      "Epoch 927 Loss: 0.1777\n",
      "Epoch 928 Loss: 0.1611\n",
      "Epoch 929 Loss: 0.1587\n",
      "Epoch 930 Loss: 0.1708\n",
      "Epoch 931 Loss: 0.1725\n",
      "Epoch 932 Loss: 0.1579\n",
      "Epoch 933 Loss: 0.1768\n",
      "Epoch 934 Loss: 0.1522\n",
      "Epoch 935 Loss: 0.1793\n",
      "Epoch 936 Loss: 0.1690\n",
      "Epoch 937 Loss: 0.1566\n",
      "Epoch 938 Loss: 0.1678\n",
      "Epoch 939 Loss: 0.1858\n",
      "Epoch 940 Loss: 0.1597\n",
      "Epoch 941 Loss: 0.1431\n",
      "Epoch 942 Loss: 0.1652\n",
      "Epoch 943 Loss: 0.1516\n",
      "Epoch 944 Loss: 0.1834\n",
      "Epoch 945 Loss: 0.1895\n",
      "Epoch 946 Loss: 0.1532\n",
      "Epoch 947 Loss: 0.1535\n",
      "Epoch 948 Loss: 0.1471\n",
      "Epoch 949 Loss: 0.1681\n",
      "Epoch 950 Loss: 0.1491\n",
      "Epoch 951 Loss: 0.1515\n",
      "Epoch 952 Loss: 0.1539\n",
      "Epoch 953 Loss: 0.1641\n",
      "Epoch 954 Loss: 0.1709\n",
      "Epoch 955 Loss: 0.1578\n",
      "Epoch 956 Loss: 0.1449\n",
      "Epoch 957 Loss: 0.1547\n",
      "Epoch 958 Loss: 0.1556\n",
      "Epoch 959 Loss: 0.1535\n",
      "Epoch 960 Loss: 0.1398\n",
      "Epoch 961 Loss: 0.1703\n",
      "Epoch 962 Loss: 0.1464\n",
      "Epoch 963 Loss: 0.1442\n",
      "Epoch 964 Loss: 0.1583\n",
      "Epoch 965 Loss: 0.1544\n",
      "Epoch 966 Loss: 0.1485\n",
      "Epoch 967 Loss: 0.1485\n",
      "Epoch 968 Loss: 0.1546\n",
      "Epoch 969 Loss: 0.1694\n",
      "Epoch 970 Loss: 0.1586\n",
      "Epoch 971 Loss: 0.1597\n",
      "Epoch 972 Loss: 0.1743\n",
      "Epoch 973 Loss: 0.1640\n",
      "Epoch 974 Loss: 0.1506\n",
      "Epoch 975 Loss: 0.1441\n",
      "Epoch 976 Loss: 0.1418\n",
      "Epoch 977 Loss: 0.1725\n",
      "Epoch 978 Loss: 0.1599\n",
      "Epoch 979 Loss: 0.1605\n",
      "Epoch 980 Loss: 0.1672\n",
      "Epoch 981 Loss: 0.1371\n",
      "Epoch 982 Loss: 0.1417\n",
      "Epoch 983 Loss: 0.1491\n",
      "Epoch 984 Loss: 0.1688\n",
      "Epoch 985 Loss: 0.1524\n",
      "Epoch 986 Loss: 0.1462\n",
      "Epoch 987 Loss: 0.1568\n",
      "Epoch 988 Loss: 0.1377\n",
      "Epoch 989 Loss: 0.1514\n",
      "Epoch 990 Loss: 0.1415\n",
      "Epoch 991 Loss: 0.1763\n",
      "Epoch 992 Loss: 0.1323\n",
      "Epoch 993 Loss: 0.1333\n",
      "Epoch 994 Loss: 0.1492\n",
      "Epoch 995 Loss: 0.1601\n",
      "Epoch 996 Loss: 0.1541\n",
      "Epoch 997 Loss: 0.1297\n",
      "Epoch 998 Loss: 0.1540\n",
      "Epoch 999 Loss: 0.1545\n",
      "Epoch 1000 Loss: 0.1639\n"
     ]
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc3b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c07fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8837209302325582"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cca11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TOG_BERT_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
